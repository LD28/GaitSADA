<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition">
  <meta name="keywords" content="domain adaptation, mmWave, self-supervised">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ekkasit.com">Ekkasit Pinyoanuntapong</a><sup>1</sup>,</span>
            <span class="author-block">
              Ayman Ali<sup>1</sup>,</span>
            <span class="author-block">
              Kalvik Jakkala<sup>1</sup>,
            </span>
            <span class="author-block">
              Pu Wang<sup>1</sup>,
            </span>
            <span class="author-block">
              Minwoo Lee<sup>1</sup>,
            </span>
            <span class="author-block">
              Qucheng Peng<sup>2</sup>,
            </span>
            <span class="author-block">
              Chen Chen<sup>2</sup>,
            </span>
            <span class="author-block">
              Zhi Sun<sup>3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of North Carolina at Charlotte,</span>
            <span class="author-block"><sup>2</sup>University of Central Florida</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2301.13384"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="https://arxiv.org/abs/2301.13384"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/exitudio/GaitSADA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://exitudio.github.io/GaitSADA/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset (under review)*</span>
                  </a>
            </div>
            <div>*The dataset is under review to ensure the preservation of the identity and privacy of individuals contained within.</div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            mmWave radar-based gait recognition is a novel user identification method that captures human gait biometrics from mmWave radar return signals. This technology offers privacy protection and is resilient to weather and lighting conditions. However, its generalization performance is yet unknown and limits its practical deployment. To address this problem, in this paper, a non-synthetic dataset is collected and analyzed to reveal the presence of spatial and temporal domain shifts in mmWave gait biometric data, which significantly impacts identification accuracy. To address this issue, a novel self-aligned domain adaptation method called GaitSADA is proposed. GaitSADA improves system generalization performance by using a two-stage semi-supervised model training approach. The first stage uses semi-supervised contrastive learning and the second stage uses semi-supervised consistency training with centroid alignment. Extensive experiments show that GaitSADA outperforms representative domain adaptation methods by an average of 15.41% in low data regimes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall Architecture of GaitSADA.</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="./assets/overall.png"
             class="interpolation-image"
             alt="Interpolate start reference image."/>
        <p>(a) Stage 1. Pretraining via semi-supervised contrastive learning to learn a
          compact gait representation from both source and domain data, which also implicitly mitigates the source-domain distribution
          shift. (b) Stage 2. fine-tuning via semi-supervised consistency training with centroid alignment. The fine-tuning stage aims
          to further improve model generalization performance by pseudo-labelling the target-domain samples, clustering together the
          samples belonging to the same class but from different domains, and pushing the class centroid close to the weight vector of
          each class. The encoders are sharing the weights.</p>
      </div>
    </div>
    <!--/ Matting. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-justified">
        <p>We curate a non-synthetic dataset consisting of mmWave radar-based gait biometric data. This dataset allows one to study and improve the spatio-temporal generalization performance of a radar-based biometric identification system</p>
        <p>We collected gait data from 10 volunteers between the
          ages of 18-35. Each subject’s data was collected in four
          different locations. The source location was a research space
          with cubicles, and the other three areas consisted of a server,
          conference, and an office room. By maintaining four distinct
          locations, we introduce SDS. In the source location, data was
          collected on 10 different days for each subject. 5 separate days
          of data was acquired for each of the three other locations,
          which are used as target domains. A participant can either
          walk towards the radar or walk away from the radar, each of
          which is counted as one walking instance and generates one
          spectrogram data sample. The data collection was limited to
          100 data samples per person in the source location and 50 data
          samples per person in each of the target locations on any given
          day</p>
        <p>Texas Instruments (TI) IWR1642EVM boost board interfaced with a DCA1000EVM board to collect
          the raw mmWave radar signal. The radar system consists of two transmitting antennas, four
          receiving antennas, and 120° view of the azimuth plane. The radar system supports up to a 4Ghz
          bandwidth operating on 77 GHz to 81 GHz. To configure FMCW wave parameters such as chirp
          width, repetition time, and chirp slope from our radar device, we use a Dell Latitude 7480 laptop
          with TI mmWave studio software as a control system.</p>
          </div>
    </div>
    <div class="column has-text-centered">
      <img src="./assets/STAR.png"
           class="interpolation-image"
           alt="Interpolate start reference image."/>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-justified">
        <p>
          The walking dataset is collected from four distinct locations such as laboratory, conference room,
          server room, and office. We recruit ten volunteers to walk in his/her natural ways. Each participant
          either walks toward or away from the radar. Data is preprocessed into 50 samples of five different
          days at each location, a total of 2500 samples (10 subjects × 50 samples × 5 days), except for the
          laboratory location, we collect additional data for another five more days (total of ten different days)
          in order to perform temporal domain shift experiments for a different time in the same location.
          We preprocess the data into 256×256×1 spectrogram and standardized values between -1 and 1.
          Laboratory location data is used as source data and the other location data (i.e. conference room,
          server room, and office) is referred to as target data. For the spatial domain drift experiment, day 1
          to day 3 samples are used as a training set for all source and target locations. The rest of the data is
          used for testing. For the temporal domain drift experiment, we employ 1 to 3 days of laboratory
          location as source data and the next consecutive days of laboratory location as target data. For
          example, considering the temporal 2-day case, the first and the second-day data of laboratory
          location is a source domain, the third and the fourth-day data is a target domain, and the fifth to
          the tenth-day data is utilized as a test set.

        </p>
      </div>
    </div>

    <div class="column has-text-centered">
      <img src="./assets/samples.png"
           class="interpolation-image"
           alt="Interpolate start reference image."/>
    </div>
    <!--/ Matting. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>If you find our work useful in your research, please consider citing:</p>
    <pre><code>@misc{pinyoanuntapong2023gaitsada,
      title={GaitSADA: Self-Aligned Domain Adaptation for mmWave Gait Recognition}, 
      author={Ekkasit Pinyoanuntapong and Ayman Ali and Kalvik Jakkala and Pu Wang and Minwoo Lee and Qucheng Peng and Chen Chen and Zhi Sun},
      year={2023},
      eprint={2301.13384},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
